[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Notebooks and other artifacts depicting our work on JGI’s citation context analsis problem\n\nHow we plan to go about tackling this problem:\n\nIdentify which publications likely are leveraging genomic data\n\nGiven a set of identifiers, can we identify which publications are incorporating those datasets?\n\nFigure out where folks are describing usage in publications\n\nCan we take the entire content of an article, identify all descriptions of data usage, and link usage descriptions to datasets being cited?\n\nFigure out how something is being used\n\nOnce we’ve determined that a citation of a dataset is valid and identified areas of an article describing usage, can we determine how something is being used in its local context within the document?\n\ni.e. ‘built a phylogenetic tree’\n\nGiven #1, #3 has to have an answer. #2 is a necessary step for determining #3 once #1 is established\n\nFigure out why something is being used\n\nCan we determine how the specific data use case relates to (a) the purpose of the study and (b) the conclusions/results of the study?"
  },
  {
    "objectID": "mistral.html",
    "href": "mistral.html",
    "title": "JGI Citation Analysis",
    "section": "",
    "text": "Code\ndef get_models(parent_name):\n    cache_dir = '/pscratch/sd/a/azaidi/llm/cache/models--'\n    models = [path.split('--')[-1] for path in \\\n              glob.glob(cache_dir + parent_name)]\n    print(f'the {parent_name} models are: {models}')\n\n\n\n\nCode\nget_models('mistralai*'), get_models('meta-llama*'); \n\n\nthe mistralai* models are: ['Mistral-7B-v0.1', 'Mixtral-8x7B-Instruct-v0.1', 'Mistral-7B-Instruct-v0.2']\nthe meta-llama* models are: ['Llama-2-7b-hf', 'Llama-2-70b-hf', 'Llama-2-70b-chat-hf']\n\n\n\n\nCode\nmodel\n\n\nMixtralForCausalLM(\n  (model): MixtralModel(\n    (embed_tokens): Embedding(32000, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MixtralDecoderLayer(\n        (self_attn): MixtralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MixtralRotaryEmbedding()\n        )\n        (block_sparse_moe): MixtralSparseMoeBlock(\n          (gate): Linear4bit(in_features=4096, out_features=8, bias=False)\n          (experts): ModuleList(\n            (0-7): 8 x MixtralBlockSparseTop2MLP(\n              (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (act_fn): SiLU()\n            )\n          )\n        )\n        (input_layernorm): MixtralRMSNorm()\n        (post_attention_layernorm): MixtralRMSNorm()\n      )\n    )\n    (norm): MixtralRMSNorm()\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)\n\n\n\n\nCode\n#model.to('cuda');\n\n\n\n\nCode\nlen(txt), tokenizer(txt).input_ids.__len__()\n\n\n(37593, 12299)\n\n\n\n\nCode\nprompt = \"The ostrich shit its pants while walking across the street. But, the Tyrannosaurus rex was one of the most ferocious predators to ever walk the Earth. With a massive body, sharp teeth, and jaws so powerful they could crush a car, this famous carnivore dominated the forested river valleys in western North America during the late Cretaceous period, 68 million years ago. \\n \\n\"\n\n\n\n\nCode\nget_response(prompt=f'what was the previous text about? \\n{prompt}')\n\n\n/pscratch/sd/a/azaidi/.conda/envs/dl/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n\n\nwhat was the previous text about? \nThe ostrich shit its pants while walking across the street. But, the Tyrannosaurus rex was one of the most ferocious predators to ever walk the Earth. With a massive body, sharp teeth, and jaws so powerful they could crush a car, this famous carnivore dominated the forested river valleys in western North America during the late Cretaceous period, 68 million years ago. \n \n\nThe previous text was about the Tyrannosaurus rex, a large carnivorous dinosaur that lived during the late Cretaceous period. The text describes its massive body, sharp teeth, and powerful jaws, and mentions that it was one of the most ferocious predators to ever walk the Earth. It does not mention the ostrich or its pants.\n\n\n\n\nCode\ntxt, pmid = call_dce(context=True)\nlen(txt), pmid\n\n\n(37593, 'PMC6000758')\n\n\n\n\nCode\n#tokenizer.encode([1,])\n\n\n\n\nCode\ntokenizer(txt,return_tensors=\"pt\")\n\n\n{'input_ids': tensor([[    1,   415, 13993,  ...,   302,  2145, 28723]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}\n\n\n\n\nCode\ndef build_prompt(questions, input_text):\n    return f\"[INST] You are a microbiology journal editor. Please answer as concisely as possible: {questions} \\n here is the text in question: {input_text} \\n [/INST]\"\n\n\n\n\nCode\nget_response(prompt=build_prompt(questions=\"can you provide the exact quote surrounding a mention of GCA_00092485.1?\",\n             input_text=call_dce(context=True)[0][:3700],),\n             new_tokens=1000)\n\n\n[INST] You are a microbiology journal editor. Please answer as concisely as possible: can you provide the exact quote surrounding a mention of GCA_00092485.1? \n here is the text in question: The ocean covers 71% of the earth’s surface and is regarded as the largest habitat for life on the planet Earth.\nMarine microorganisms are known to play an essential role in energy conservation and biogeochemical cycling in the oceans.\nHeterotrophic prokaryotes are considered key players in the decomposition of the dissolved organic matter (DOM) and particulate organic matter (POM) present therein (DeLong and Karl, 2005; Azam and Malfatti, 2007).\nOceanospirillales is an order of proteobacteria with seven families of heterotrophic marine bacteria that are usually associated with oil spills and are known to be involved in xylan and hydrocarbon utilization (Choi et al., 2012; Cao et al., 2014).\nRecently, a new family named Kangiellaceae has been proposed within the order Oceanospirillales based on phylogenetic, chemotaxonomic and physiological characteristics; this family comprises three genera: Kangiella, Aliikangiella, and Pleionea (Wang et al., 2015).\nAlthough the Kangiella genus was reclassified within Kangiellaceae instead of Alcanivoracaceae based on 16S rRNA gene phylogeny (Wang et al., 2015), taxonomic signatures at the genomic level are still needed.\nKangiella are gram-negative long rods that are non-motile and non-spore-forming bacteria.\nNine strains belonging to the genus Kangiella have been isolated from various marine environments, including marine sand (Kim et al., 2015), tidal flat sediments and coastal regions (Yoon et al., 2004, 2012; Romanenko et al., 2010; Ahn et al., 2011; Jean et al., 2012; Lee et al., 2013).\nFor example, both K. koreensis DSM 16069 and Kangiella aquimarina DSM 16071 were isolated from tidal flat sediment at Daepo Beach, Yellow Sea, Korea, and were shown to grow optimally at 30–37°C and pH 7.0–8.0 (Yoon et al., 2004).\nK. sediminilitoris KCTC 23892 was isolated from tidal flat sediment from the South Sea in South Korea, which was shown to grow optimally at 30–37°C and pH 7.0–7.5 (Lee et al., 2013).\nKangiella geojedonensis KCTC 23420 was isolated from seawater off the southern coast of Korea, and was shown to grow optimally at 10–40°C and pH 7.0–7.5 (Yoon et al., 2004).\nRecently, we isolated a novel strain named Kangiella profundi from a deep sea sediment sample that was collected from the southwest Indian Ocean at a depth of 2784 m (Xu et al., 2015).\nDue to limited research, our understanding about the metabolic potential and ecological functions of bacteria belonging to the family Kangiellaceae is still obscure.\nTo date, no genome has been completely sequenced from either of the two genera Aliikangiella and Pleionea.\nMoreover, only three completely sequenced genomes of the genus Kangiella, i.e., K. koreensis DSM 16069 (GenBank accession: GCA_000024085.1), K. sediminilitoris KCTC 23892 (GenBank accession: GCA_001708405.1), and K. geojedonensis KCTC 23420 (GenBank accession: GCA_000981765.1), are available.\nIn this study, we report the complete genome sequence of K. profundi FT102 (GenBank accession: CP025120).\nComparative genomics-based approaches were utilized to explore the metabolic potential of Kangiella.\nWe show that featured genomic reduction in Kangiella is due to low genomic redundancy as well as Kangiella lineage-specific gene loss.\nThe metabolic base of Kangiella as a powerful extracellular protein degrader was investigated.\nOur study provides the first insights into the genomic and metabolic capabilities of Kangiella.\nKangiella profundi FT102T ( = CGMCC 1.12959T = KCTC 42297T = JCM 30232T) is from our laboratory stock.\nThe strains K. koreensis DSM 16069 (JCM 12317), K. aquimarina DSM 16071 (JCM 12318) and K. geojedonensis KCTC 23420 were obtained from Japan Collection of Microorganisms (J \n [/INST] The exact quote surrounding the mention of GCA_000924085.1 is: \"only three completely sequenced genomes of the genus Kangiella, i.e., K. koreensis DSM 16069 (GenBank accession: GCA_000024085.1), K. sediminilitoris KCTC 23892 (GenBank accession: GCA_001708405.1), and K. geojedonensis KCTC 23420 (GenBank accession: GCA_000981765.1), are available.\" This indicates that the genome of K. koreensis DSM 16069, with GenBank accession GCA_000024085.1, has been completely sequenced.\n\n\n\n\nCode\n#get_response(prompt=f'can you provide the exact quote surrounding a mention of GCA_000024085.1? Please provide the sentence before and after. \\n{txt} -------\\n\\n')\n\n\n\n\nCode\n#mistral prompt format\n# &lt;s&gt;[INST] Instruction [/INST] Model answer&lt;/s&gt;[INST] Follow-up instruction [/INST]\n\n\n\n\nCode\n#prompt = f'&lt;s&gt;[INST] Can you summarize this text? {sample_text} [/INST]'\n#prompt = f'&lt;s&gt;[INST] {text} [/INST]'\n\n\n\n\nCode\nprompt = \"can you tell me about yourself?\"\n\n\n\n\nCode\nmodel_inputs = tokenizer([prompt], return_tensors=\"pt\").to('cuda');\n\n\n\n\nCode\nlen(model_inputs.input_ids[0])\n\n\n8\n\n\n\n\nCode\ngenerated_ids = model.generate(**model_inputs, max_new_tokens=250,)#do_sample=True)\ntokenizer.batch_decode(generated_ids)[0]\n\n\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n\n\n'&lt;s&gt; can you tell me about yourself?\\n\\nI’m a 20-year-old college student from the United States. I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and I’m currently studying at the University of California, Berkeley. I’m a student of the arts, and'\n\n\n\n\nCode\n#model_inputs.input_ids\ntoken_dict = dict(zip(tokenizer.vocab.values(), tokenizer.vocab.keys()))\n\n\n\n\nCode\nimport pandas as pd\n\n\n\n\nCode\nmodel_inputs.input_ids.to('cpu').shape\n\n\ntorch.Size([1, 20533])\n\n\n\n\nCode\nsample_text_tokens = pd.Series(model_inputs.input_ids.to('cpu').squeeze()).map(token_dict)\nsample_text_tokens.head(5)\n\n\n0     &lt;s&gt;\n1     &lt;s&gt;\n2      ▁[\n3    INST\n4       ]\ndtype: object\n\n\n\n\nCode\nsample_text_tokens[:50]\n\n\n0         &lt;s&gt;\n1         &lt;s&gt;\n2          ▁[\n3        INST\n4           ]\n5        ▁Can\n6        ▁you\n7        ▁sum\n8           m\n9         ara\n10        ize\n11      ▁this\n12      ▁text\n13          ?\n14          ▁\n15     &lt;0x0A&gt;\n16         ▁▁\n17     &lt;0x0A&gt;\n18         ▁▁\n19     &lt;0x0A&gt;\n20       ▁▁▁▁\n21     &lt;0x0A&gt;\n22      ▁▁▁▁▁\n23     ▁Front\n24     ▁Micro\n25         bi\n26         ol\n27     &lt;0x0A&gt;\n28      ▁▁▁▁▁\n29     ▁Front\n30     ▁Micro\n31         bi\n32         ol\n33     &lt;0x0A&gt;\n34      ▁▁▁▁▁\n35     ▁Front\n36          .\n37     ▁Micro\n38         bi\n39         ol\n40          .\n41     &lt;0x0A&gt;\n42     ▁▁▁▁▁▁\n43     &lt;0x0A&gt;\n44    ▁▁▁▁▁▁▁\n45     ▁Front\n46       iers\n47        ▁in\n48     ▁Micro\n49         bi\ndtype: object"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring the world of JGI Data Citations",
    "section": "",
    "text": "First we’ll start off by feeding our data through an LLM Mistral\n\n This was built with Quarto"
  }
]